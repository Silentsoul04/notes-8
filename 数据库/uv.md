- [多维分析中的 UV 与 PV ](https://www.sohu.com/a/115979730_116235)

# “可加”与“不可加”

正如上面提到的，多维分析对于查询速度非常敏感，业内也有很多专门的存储和查询方案。

而在具体的实现中，有一种最为常见的实现手段，就是把各个维度的所有取值组合下的指标全部预先计算并且存储好，这种一般可以称作事实表。然后在具体进行多维查询的时候，再根据维度的选择，扫描相对应的数据，并聚合得到最终的查询条件。

此时，会发现一个比较有意思的问题，就是 PV 这类指标，是“可加”的，而 UV 这类指标，则是“不可加”的

# UV 计算的常见方案

UV 类型的指标，有三种常见的计算方案，我们在这里分别进行介绍。

## 估算方案

所谓的估算方案，就是在上面的表格的基础上，不再额外记录更多细节，而是通过估算的方式来给出一个接近真实值的 UV 结果，常见的算法有很多，例如 HyperLogLog 等。

由于毕竟是估算，最终估算的结果有可能与真实值有较大差异，因此只有一些统计平台可能会采用，而如我们 Sensors Analytics 之类的以精细化分析为核心的分析系统并不会采用，因此在这里不做更多描述。

## 扩充事实表，以存代算

所谓以存代算，就是在预先计算事实表的时候，将所有需要聚合的结果，都算好。

依然以上面的例子来说明，如果我们想以存代算，预先做完聚合，类似于 Hive 所提供的group by with cube操作。

> 无法进行多选，除非把多选也作为维度。维度爆炸

## 从最细粒度数据上扫描

之前提出的扩充事实表的方式，的确可以解决多维分析中指标聚合的问题，除此之外，还有一种方案，则是在事实表上，将用户ID也做为一个维度，来进行保存，此时就不需要保存 UV 了

虽然这样一来，需要保存的数据规模有了数量级上的扩充，并且所有的聚合计算都需要在多维分析查询的时候再扫描数据并进行聚合，存储和计算代价都提高了很多，看似是一种很无所谓的举措。

但是，相比较之前的方案，它却有一个最大的好处，也即是因为有了最细粒度的用户行为数据，才有可能计算事件级别的漏斗、留存、回访等，才有可能在这些数据的基础之上，进一步做用户画像、个性化推荐等等。而这也正是目前 Sensors Analytics 所采用的数据存储方案，也正因为采用了这种存储方案，我们才能够将自己成为精细化用户行为分析系统，才能够满足使用者的最细粒度数据分析和获取的需求。


## 小结
在这样一个数据存储方案的基础上，为了提高数据查询的效能，一般的优化思路有**采用列存储加压缩的方式减少从磁盘中扫描的数据量**，采用分布式的方案提高并发扫描的性能，采用**应用层缓存来减少不同查询的公共扫描数据的量等等**，这方面的内容我们会在后面的文章里面做进一步的探讨，尽请期待。

> 位图属于最细粒度数据上的扫描。但不是采用列存储压缩，而是用了位图进行压缩。

---

-  [Flink 在快手实时多维分析场景的应用](https://www.infoq.cn/article/zkz1vpe3qgyfrutb6pcm)

# 快手实时多维分析平台

## 对比
现在社区已经有一些 OLAP 实时分析的工具，像 Druid 和 ClickHouse；目前快手采用的是 Flink+Kudu 的方案，在前期调研阶段对这三种方案从计算能力、分组聚合能力、查询并发以及查询延迟四个方面结合实时多维查询业务场景进行对比分析：

- 计算能力方面：多维查询这种业务场景需要支持 Sum、Count 和 count distinct 等能力，而 Druid 社区版本不支持 count distinct，快手内部版本支持数值类型、但不支持字符类型的 count distinct；ClickHouse 本身全都支持这些计算能力；Flink 是一个实时计算引擎，这些能力也都具备。
- 分组聚合能力方面：Druid 的分组聚合能力一般，ClickHouse 和 Flink 都支持较强的分组聚合能力。
- 查询并发方面：**ClickHouse 的索引比较弱，不能支持较高的查询并发**，Druid 和 Flink 都支持较高的并发度，存储系统 Kudu，它也支持强索引以及很高的并发。
- 查询延迟方面：Druid 和 ClickHouse 都是在查询时进行现计算，而 Flink+Kudu 方案，通过 Flink 实时计算后将指标结果直接存储到 Kudu 中，查询直接从 Kudu 中查询结果而不需要进行计算，所以查询延迟比较低。

以 UV 类指标计算举例，两个黄色虚线框分别对应两层计算模块：全维计算和降维计算。

## 全维计算

全维计算分为两个步骤，为避免数据倾斜问题，首先是**维度打散预聚合，将相同的维度值先哈希打散一下**。因为 UV 指标需要做到精确去重，所以采用 Bitmap 进行去重操作，每分钟一个窗口计算出增量窗口内数据的 Bitmap 发送给第二步按维度全量聚合；在全量聚合中，将增量的 Bitmap 合并到全量 Bitmap 中最终得出准确的 UV 值。然而有人会有问题，针对用户 id 这种的数值类型的可以采用此种方案，但是对于 deviceid 这种字符类型的数据应该如何处理？实际上在源头，数据进行维度聚合之前，会通过字典服务将字符类型的变量转换为唯一的 Long 类型值，进而通过 Bitmap 进行去重计算 UV。

降维计算中，通过全维计算得出的结果进行预聚合然后进行全量聚合，最终将结果进行输出。
