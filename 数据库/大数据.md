---
[大数据的那些事](https://www.zhihu.com/people/fei-zong-55/posts?page=6)

作者的观点：

## 2
GFS的设计理念上做了两个非常重要的假设，其一是这个文件系统只处理大文件，一般来说要以TB或者PB作为级别去处理。其二是这个文件系统不支持update只支持append。在这两个假设的基础上，文件系统进一步假设可以把大文件切成若干个chunk，本文上面的图大致上给了GFS的一个基本体系框架的解释。

Chunk server是GFS的主体，它们存在的目的是为了保存各种各样的chunk。这些chunk代表了不同文件的不同部分。为了保证文件的完整性不受到机器下岗的影响，通常来说这些chunk都有冗余，这个冗余标准的来说是3份。有过各种分析证明这个三份是多门的安全。

除了保存实际数据的chunk server以外，我们还需要metadata manager，在GFS里面这个东西就是master了。

Master的重要性不言而喻。没有了metadata的文件系统就是一团乱麻。Google的实现实际上用了一个Paxos协议，倘若我的理解是正确的话。Paxos是Lamport提出来的用来解决在不稳定网络里面的consensus的一个协议。

对GFS的访问通过client，读的操作里，client会从master那边拿回相应的chunk server，数据的传输则通过chunk server和client之间进行。不会因此影响了master的性能。而写的操作则需要确保所有的primary以及secondary都写完以后才返回client。如果写失败，则会有一系列的retry，实在不行则这些chunk会被标注成garbage，然后被garbage collection。


## 3
我们先看看作为编程模型的MapReduce。所谓MapReduce的意思是任何的事情只要都严格遵循Map Shuffle Reduce三个阶段就好。其中Shuffle是系统自己提供的而Map和Reduce则用户需要写代码。Map是一个per record的操作。任何两个record之间都相互独立。Reduce是个per key的操作，相同key的所有record都在一起被同时操作，不同的key在不同的group下面，可以独立运行。

那么我们来看看为什么Google可以做到那么大规模的数据处理。首先这个系统的第一条，很简单，所有的中间结果可以写入到一个稳定的，不因为单机的失败而不能工作的分布式海量文件系统。GFS的伟大可见一斑。没有GFS，玩你妹的MapReduce。没有一个database厂商做出过伟大的GFS，当然也就没办法做出这么牛叉的MapReduce了。

这个系统的第二条也很简单，能够对单个worker进行自动监视和retry。这一点就使得单个节点的失败不是问题，系统可以自动的进行管理。加上Google一直保持着绝不泄密的资源管理系统Borg。使得Google对于worker能够进行有效的管理。


##
在BigTable的实现上，一个BigTable的cluster由一个client library，一个Master server和很多个的Tablet Server组成。按照论文的说法，一个大的BigTable会被分成若干个大小大致在100MB到200MB的tablets，而这些tablets 会被分配到一些Tablet Server上去给client 提供服务来。Tablet Server对超过200MB的tablet灰进行切分，对小于100MB的则会进行合并。

系统运行过程中的Tablet server的数量不是固定的，可以根据实际上的工作负载来增加或者减少，这方面的工作是Master server来控制的。Tablet server并不存储实际的文件，而是作为一种service和proxy来访问存在Google File System里的实际的tablet们。

和Tablet server不一样的是，Master Server始终都存在。Master server存在主要是把tablet分配给Tablet server，增加或者减少Tablet Server，并且负责去平衡不同的Tablet server的load。如果用户要创建一个新的Table，或者对已经有的Table做改动的话，譬如增加新的column family等，都是通过Master server来完成的。Master server通过Tablet server来实现对tablet的间接操作，本身并不负责对任何Tablet 的管理。

和大家直观上想象的不同，当一个client要访问BigTable的时候，client并不需要和Master server进行交流。这就保证了Master server的load并不是很重。那么，client是怎么样实现对BigTable的访问的呢? 这是BigTable比较精密的difference。这需要用到Chubby。

Chubby是一个highly distributed lock service。可以认为开源Zookeeper是一个Chubby的copycat。但是虽然说是CopyCat，实际上Chubby实现的是一个Paxos协议，而Zookeeper实现的是它的变种Zab。这方面我们不展开细讲。 具体的情况请阅读 The Chubby lock service for loosely-coupled distributed systems。

Chubby实现的是一个类似文件系统那样的目录结构。使用者可以访问这些文件来获得对被访问对象的锁。按照BigTable论文的说法，Chubby的用处有很多处，包括对Tablet的定位，对Tablet server的监控等等。

在BigTable里， SSTable(Sorted Strings Table)是一个基本的单元。每个Tablet有若干个SSTable。LSM-Tree

整个SSTable的实现分为memTable和磁盘上的SSTable。在内存里使用的是skip-list。所以写的操作只是写内存，非常的快。而内存写满之后就会把这个memTable变成一个immutable的memTable。同时开一个新的可以写的memTable另外一个线程则会把这个immutable的内存表变成一个磁盘上的SSTable。当这个转变完成以后immutable的内存表被释放。如此往复磁盘上会产生很多的SSTable。这就需要compact。SSTable是有level1， level2，。。。的。其中进到level1的compact叫做minor compact。后面还有major compact。从level2开始以后任意两棵树的key之间不会有overlap，但是在level1这并不guarantee。

所以我们的一个读操作要读memTable，immutable memTable，level1的tree，和level2以及以下的level的1棵树。这说明读的操作相对写的操作会更贵一些。大家需要注意的是，如果我们访问的是最新的版本，那么有可能会在内存里，所以这个设计对于读的操作主要是优化了新版本的读。对于cool data的读则要慢很多。顺便补充一句，Facebook的copycat RocksDB和LevelDB最主要的区别据说是引入了一个叫做universal compact的东西。当然我没有研究过这个codebase，不清楚universal compact到底有多牛。


当然，就像任何一个类似的系统一样，BigTable的recovery基于log，所有的写操作进内存之前写进log。LevelDB的log format并不是太难懂，是经典的append only的操作。基于log的读写恢复是任何一个系统的基础了。我就不再展开叙述。
