# ClickHouse 数据压缩与解压

那么为什么LZ4解压缩成为一个瓶颈呢？LZ4看起来是一种非常轻的算法:数据解压缩速率通常是每个处理器内核1到3 GB/s，具体取决于数据。这比典型的磁盘子系统快得多。此外，我们使用所有可用的中央处理器内核，解压缩在所有物理内核之间线性扩展。

首先，如果数据压缩率很高，则磁盘上数据占用空间就很小，在读取数据时磁盘IO会比较低，但是如果待解压的数据量很大则会影响到CPU使用率。在LZ4的情况下，解压缩数据所需的工作量几乎与解压缩数据本身的量成正比；其次，如果数据被缓存，你可能根本不需要从磁盘读取数据。可以依赖页面缓存或使用自己的缓存。缓存在面向列的数据库中更有效，因为只有经常使用的列保留在缓存中。这就是为什么LZ4经常成为CPU负载的瓶颈。


- [ClickHouse 在趣头条的实践](https://mp.weixin.qq.com/s/lP9quNJuhpXHxP-n8W0maw)
- [ClickHouse 数据压缩与解压](https://knifefly.cn/2019/08/25/ClickHouse%E5%8E%8B%E7%BC%A9%E4%B8%8E%E8%A7%A3%E5%8E%8B/)
- [How to speed up LZ4 decompression in ClickHouse](https://habr.com/en/company/yandex/blog/457612/)


# 如何进行分区的覆盖的

## 背景
更新场景，但是更新的效率不高，所以每天会进行全量复制的更改

## 问题

如果直接drop掉分区，后写入。会在当前写入进度中导致数据查询的异常

## 解决办法

通过影子表的方式进行分区覆盖

1. 创建一个结构一样的影子表
2. 往影子表进行数据的写入
3. 写入完成后，通过replace partition操作进行数据的替换
4. 删除旧分区的数据。如果需要保存旧的状态表，进行数据归档和淘汰策略。



---
# 分片与复制

需要提醒一下，每个clickhouse-server实例只能放一个分片的一个备份，也就是3分片2备份需要6台机器（6个不同的clickhouse-server）。

之前为了节省资源，打算循环使用，把shard1的两个副本放到hadoop1、hadoop2两个机器上，shard2的两个副本放到hadoop2、hadoop3上，shard3的两个副本放到hadoop3、hadoop1上，结果是不行的。

原因是shard+replica对应一个数据表，Distributed查询规则是每个shard里找一个replica，把结果合并。



## 禁止分布式写入，采用本地表写入。

社区很多伙伴在分享时，也都提到了禁止使用分布式表写入。我们也一样。

禁止使用的原因是需要 设计及减少Part的生成频率。这对整个集群的稳定性和整体性能有着决定的作用。这个在之前我司的分享中曾经介绍过。我们控制批次的上线和批次的时间窗口。保障写入操作对每个节点的稳定压力。

- [insert data via the distributed table or local table](https://github.com/ClickHouse/ClickHouse/issues/1854#issuecomment-363197252)

原因：
- 它更有效，因为它避免了临时数据的过度复制。
- 它更灵活，因为您可以使用任何复杂的切分模式，而不仅仅是简单的按除法模进行的切分。
注意事项：
- 插入到本地表需要客户机应用程序上更多的逻辑，并且可能更难使用。但它在概念上更简单。
- 如果您的查询依赖于一些关于数据分布的假设，比如使用IN或JOIN（连接同一位置的数据）而不是GLOBAL IN、GLOBAL JOIN的查询，那么您必须自己维护正确性。


### 写分布式表

数据写入时，先写入集群中的分布式表下的节点临时目录，再由分布式表将 Insert 语句分发到集群各个节点上执行，分布式表不存储实际数据。

ClickHouse 在分布式写入时，会根据节点数量在接收请求的节点的下创建集群节点的临时目录，数据（Insert 语句）会优先提交的本地目录下，之后同步数据到对应的节点。此过程好处是提交后，数据不会丢失。我们模拟同步过程中节点异常，重启后数据也会自动恢复。如果你的数据量及集群压力并不大，分布式也可以认为是一种简单的实现方式。


- [Clickhouse集群应用、分片、复制](https://www.jianshu.com/p/20639fdfdc99)
- [百分点大数据技术团队：ClickHouse国家级项目最佳实践](http://blog.itpub.net/69965230/viewspace-2690052/)
- [ClickHouse万亿数据双中心的设计与实践](https://cloud.tencent.com/developer/article/1530809)
