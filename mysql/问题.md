[toc]

---
## 分区操作锁查
从库有慢查询，alter不了的话，后面的查询就会处于lock wait


---

## 慢查询

```
select hash_val, sum(freq) as s_f, flag from app_keyword_freq where created_time between '2018-08-02' and '2019-01-28' and ((campaign_type = 201 and campaign_id in (262,1239,6423,2398578,2419409)) or (campaign_type = 101 and campaign_id in (171235,270955))) group by hash_val order by s_f desc limit 100
```

问题： 
原因： 分区过多导致, 并且每个分区的数目较小，速度都去查分区去了
待评估： 是不是涉及到查180天的分区的相关语句也会有相应的问题？
更新原因： 测试后发现是buffer资源不足(见下面说明)，不是分区问题


## 慢查询

```

select advertisement.id, campaign_id, count(distinct(advertisement.id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids, count(distinct(ad_log_summary.createdAt)) as duration, count(*) from advertisement right join ad_log_summary on (advertisement.id = ad_log_summary.adid  and  ad_log_summary.createdAt >= "2018-09-01"  and  ad_log_summary.createdAt < "2019-01-30" )  where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and campaign_type=101 and purpose = 1  and platform =  2 group by campaign_id;


select ad_id, campaign_id, count(distinct(ad_id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids from ad_aggs_outer where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and purpose = 1  and platform =  2 and campaign_type = 101 and ad_year_month in (1811, 1901, 1810, 1806) 
group by campaign_id;
```

```sql
SELECT GROUP_CONCAT(id) from ( SELECT id FROM `campaign_record` where type_id = 101 ORDER by rand() LIMIT 20) as t;

select hash_val, sum(freq) as s_f, flag from app_keyword_freq where created_time between '2018-08-02' and '2019-01-28' and ((campaign_type = 201 and campaign_id in (262,1239,6423,2398578,2419409)) or (campaign_type = 101 and campaign_id in (171235,270955))) group by hash_val order by s_f desc limit 100;

select ad_id, campaign_id, count(distinct(ad_id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids from ad_aggs_outer where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and purpose = 1  and platform =  2 and campaign_type = 101 and ad_year_month in (1811, 1901, 1810, 1806) 
group by campaign_id;

```
问题： 命中索引并且数目较小，第一次查很慢，下一次查就很

原因： 机器的buffer不足导致，查询索引的时候就需要磁盘io把所需的索引页load进buffer里面
解决办法： 把测试库迁移出去/升级机器


---
# 锁表
问题： alter被慢查询锁住了
原因： 问题相关查询，是在进行一些alter table等DDL操作时，如果该表上有正在进行的操作(包括读)，则无法获取metadata 独占锁，会阻塞。还会阻塞后续的查询操作(?)

## 其他卡死原因
- 未提交事务，阻塞DDL，继而阻塞所有同表的后续操作。

MySQL在对DDL操作：

    1：mysql 5.6支持在线ddl操作，在对表进行alter table 时，对该表的增，删，改，查均不会锁表。（我线上用的是：MySQL 5.6.26）

    2：但如果在该表被访问时，执行DDL操作会导致表锁，会阻塞对表的任何操作（所以在进行上线操作时一定要观察一下是否有对表操作的慢的查询语句或者事务）

    3：如果有慢查询 -> 只有慢查询结束了之后，才能创建索引和其他的操作

注意：DDL操作尽量在业务低峰时候操作，以免影响正常业务

## 参考链接
* [sql卡死](https://1181731633.iteye.com/blog/2330291)
* [metadata-locking](https://dev.mysql.com/doc/refman/5.5/en/metadata-locking.html)
* [MySQL创建索引导致锁及表锁阻塞查询](https://blog.csdn.net/leyangjun/article/details/100115400)

---
# mysql并发重复插入

原因： get or create高并发是重复插入了

方案：

1. 应该需要ignore？

2. django 的orm应该是 get_or_create，好像也是无法解决, get_or_create也是先select后事务插入

3. select...for update？查询加锁？

4. from threading import Lock  不行，这是线程锁

5. uwsgi的锁 https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/Locks.html

## select...for update
- 测试后发现仅仅对主键行表锁，对非主键的唯一索引全表锁;
- 并且不会对查不到的行进行锁;

|      事务1            | 事务2| 
| ----------------- | ---------------------------- |
|set autocommit=0; | |
| | set autocommit=0;|
| select * from auth_user where id = 2 for update; # empty set| |
| | select * from auth_user where id = 2 for update; # empty set|

| 事务1                 | 事务2                        | 
| ----------------- | ---------------------------- |
|set autocommit=0; | |
| | set autocommit=0;|
| select * from auth_user where username = 18819423815 for update;| |
| | select * from auth_user where username = 10000000006 for update; # wati util commit |
所以不能解决

## 参考链接
* [How do I deal with this race condition in django?](https://stackoverflow.com/questions/2235318/how-do-i-deal-with-this-race-condition-in-django)


---
增量更新疑问：

是假设这一秒之间内取了数据，但是这一秒又插入了数据。这时候select的这一秒会有遗漏，需要进行读写整体开事务。
select ... from tbl where modify_time > @last_modify_time and modify_time <= now();
那能通过now() - 1秒 去解决么？

每小时跑该小时的数据, 具体也有类似的错误？

如何处理事务里的数据？事务数据提交的modify_time是旧的？导致遗漏？

A： 长事务会造成遗漏

---

用上索引UNIQUE `username` (`username`) USING BTREE;
```
explain select *
from auth_user
where  auth_user.username in ("15068102426")
limit 10

where  auth_user.username in (15068102426) 用不上索引!!类型转换？！


explain select *
from auth_user inner JOIN 
     auth_user_baseinfo on ( auth_user.id = auth_user_baseinfo.uid )
where  auth_user.username in ("15068102426") or auth_user_baseinfo.outer_id in ("b7904d23-e729-39af-9990-e83a090c6f03", "c9602a01-8761-3f3c-87b7-c91eee1b28d5")
limit 10
```
join后的or 操作是用不上索引的任意一个都不行。