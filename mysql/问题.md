[toc]

---
## 分区操作锁查
从库有慢查询，alter不了的话，后面的查询就会处于lock wait


---

## 慢查询

```
select hash_val, sum(freq) as s_f, flag from app_keyword_freq where created_time between '2018-08-02' and '2019-01-28' and ((campaign_type = 201 and campaign_id in (262,1239,6423,2398578,2419409)) or (campaign_type = 101 and campaign_id in (171235,270955))) group by hash_val order by s_f desc limit 100
```

问题： 
原因： 分区过多导致, 并且每个分区的数目较小，速度都去查分区去了
待评估： 是不是涉及到查180天的分区的相关语句也会有相应的问题？
更新原因： 测试后发现是buffer资源不足(见下面说明)，不是分区问题


## 慢查询

```

select advertisement.id, campaign_id, count(distinct(advertisement.id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids, count(distinct(ad_log_summary.createdAt)) as duration, count(*) from advertisement right join ad_log_summary on (advertisement.id = ad_log_summary.adid  and  ad_log_summary.createdAt >= "2018-09-01"  and  ad_log_summary.createdAt < "2019-01-30" )  where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and campaign_type=101 and purpose = 1  and platform =  2 group by campaign_id;


select ad_id, campaign_id, count(distinct(ad_id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids from ad_aggs_outer where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and purpose = 1  and platform =  2 and campaign_type = 101 and ad_year_month in (1811, 1901, 1810, 1806) 
group by campaign_id;
```

```sql
SELECT GROUP_CONCAT(id) from ( SELECT id FROM `campaign_record` where type_id = 101 ORDER by rand() LIMIT 20) as t;

select hash_val, sum(freq) as s_f, flag from app_keyword_freq where created_time between '2018-08-02' and '2019-01-28' and ((campaign_type = 201 and campaign_id in (262,1239,6423,2398578,2419409)) or (campaign_type = 101 and campaign_id in (171235,270955))) group by hash_val order by s_f desc limit 100;

select ad_id, campaign_id, count(distinct(ad_id)) as adverts, count(distinct(channel_id)) as channels, GROUP_CONCAT(DISTINCT (channel_id)) AS c_ids from ad_aggs_outer where campaign_id in (201857,172034,343939,263812,163587,257416,1601545,502286,162703,276881,162838,331286,176156,182565,159528,4679085,191922,271413,168119,205626,5448507,1453372,1603515,7492410,7440705,159816,197200,5664853,7452889,7554146,164326,163581,5505897,1625838,180722,1623797,1407353)  and purpose = 1  and platform =  2 and campaign_type = 101 and ad_year_month in (1811, 1901, 1810, 1806) 
group by campaign_id;

```
问题： 命中索引并且数目较小，第一次查很慢，下一次查就很

原因： 机器的buffer不足导致，查询索引的时候就需要磁盘io把所需的索引页load进buffer里面
解决办法： 把测试库迁移出去/升级机器


---
# 锁表
问题： alter被慢查询锁住了
原因： 问题相关查询，是在进行一些alter table等DDL操作时，如果该表上有正在进行的操作(包括读)，则无法获取metadata 独占锁，会阻塞。还会阻塞后续的查询操作(?)

## 其他卡死原因
- 未提交事务，阻塞DDL，继而阻塞所有同表的后续操作。

## 参考链接
* [sql卡死](https://1181731633.iteye.com/blog/2330291)
* [metadata-locking](https://dev.mysql.com/doc/refman/5.5/en/metadata-locking.html)


---
# mysql并发重复插入

原因： get or create高并发是重复插入了

方案：

1. 应该需要ignore？

2. django 的orm应该是 get_or_create，好像也是无法解决, get_or_create也是先select后事务插入

3. select...for update？查询加锁？

4. from threading import Lock  不行，这是线程锁

5. uwsgi的锁 https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/Locks.html

## select...for update
- 测试后发现仅仅对主键行表锁，对非主键的唯一索引全表锁;
- 并且不会对查不到的行进行锁;

|      事务1            | 事务2| 
| ----------------- | ---------------------------- |
|set autocommit=0; | |
| | set autocommit=0;|
| select * from auth_user where id = 2 for update; # empty set| |
| | select * from auth_user where id = 2 for update; # empty set|

| 事务1                 | 事务2                        | 
| ----------------- | ---------------------------- |
|set autocommit=0; | |
| | set autocommit=0;|
| select * from auth_user where username = 18819423815 for update;| |
| | select * from auth_user where username = 10000000006 for update; # wati util commit |
所以不能解决

## 参考链接
* [How do I deal with this race condition in django?](https://stackoverflow.com/questions/2235318/how-do-i-deal-with-this-race-condition-in-django)


---
增量更新疑问：

是假设这一秒之间内取了数据，但是这一秒又插入了数据。这时候select的这一秒会有遗漏，需要进行读写整体开事务。
select ... from tbl where modify_time > @last_modify_time and modify_time <= now();
那能通过now() - 1秒 去解决么？

每小时跑该小时的数据, 具体也有类似的错误？

如何处理事务里的数据？事务数据提交的modify_time是旧的？导致遗漏？