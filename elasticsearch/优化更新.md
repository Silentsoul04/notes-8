# 写入优化

- 如果是集群**首次灌入数据**,可以**将副本数设置为0**，写入完毕再调整回去，这样副本分片**只需要拷贝**，节省了**索引过程**/如果要进行**大量批量导入**，请考虑通过设置index.number_of_replicas来禁用副本：0。

主要原因在于：复制文档时，将整个文档发送到副本节点，并逐字重复索引过程。这意味着每个副本都将执行**分析，索引和潜在合并过程**。

相反，如果使用零副本进行索引，然后在提取完成时启用副本，则恢复过程本质上是逐字节的网络传输。 这比复制索引过程更有效。


参考链接：

- https://www.jianshu.com/p/f67b046b4d3f
- https://www.easyice.cn/archives/207#refresh_interval
- https://kuaibao.qq.com/s/20180405G021ZA00?refer=spider

---
# refresh_interval
设置刷新时间
```
PUT /ag_advertisement_test/_settings
{
    "index" : {
        "refresh_interval" : "-1"
    }
}
```
设置后，更新并不能通过_search是获取到最新的数据，
但是如果：**直接去获取单个文档的数据**（`GET ag_advertisement_test/data/1000003?parent=1900d3a59d00c93b26678287e6df0185`），
会发现最新的数据.并且发现_search的**部分数据已经刷新到最新**。

而且如果再次再次更新单个文档的， 会把旧的版本刷新到可搜素，但仍不是最新的，而且也发现_search的部分数据已经刷新到最新

> 原因见事务日志的作用

---
## 亚马逊的建议
分片的一般大小推荐为:30G

如何提高我的 Elasticsearch 集群上的索引性能？

- 将 refresh_interval 提高到 60 秒或以上

- 将副本数量更改为零

- 找到最佳批量请求大小的实验：从 5–15 MiB 的批量请求大小开始。然后，缓慢增加请求大小，直到索引性能停止改进。

- 缩小响应大小。通过filter_path进行返回信息的缩减。减少网络传输的流量。

- 提高 index.translog.flush_threshold_size 的值
> 默认情况下，index.translog.flush_threshold_size 被设置为 512 MB。这表示当 translog 达到 512 MB 时会被刷新。索引负载越繁重，translog 刷新就越频繁。当您提高 index.translog.flush_threshold_size 时，节点执行此开销大的操作的频率较低。这通常会提高索引性能。提高大小的另一个益处在于，集群会创建几个大型分段，而不是多个小型分段。大型分段的合并频率较低，这意味着更多的线程将用于索引而不是合并。
提高 index.translog.flush_threshold_size 的缺点在于，translog 刷新需要更长时间。如果某个分区失败，由于 translog 较大，恢复需要的时间更长。

> 刷新阈值大小设置为 1024 MB，这非常适合内存超过 32 GB 的实例。选择最适合您的使用案例的阈值大小。

- 禁用 _all 字段。


参考链接：
- [如何提高我的 Elasticsearch 集群上的索引性能？](https://aws.amazon.com/cn/premiumsupport/knowledge-center/elasticsearch-indexing-performance/)
- [如何排查 Amazon Elasticsearch Service 集群上的高 JVM 内存压力问题？](https://amazonaws-china.com/cn/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/)
- [Reducing Response Size](https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-indexing.html#es-indexing-size)


---
# 段合并

## 优化点一：降低分段产生的数量 / 频率

- 可以将 Refresh Interval 调整到分钟级别 /indices.memory.index_buffer_size (默认是 10%)
- 尽量避免文档的更新操作

## 优化点二：降低最大分段大小，避免较大的分段继续参与 Merge，节省系统资源。（最终会有多个分段）

- Index.merge.policy.segments_per_tier，默认为 10， 越小需要越多的合并操作
- Index.merge.policy.max_merged_segment, 默认 5 GB， 操作此大小以后，就不再参与后续的合并操作
> segment size; bigger segments won’t be merged with other segments. You’d lower this value if you wanted less merging and faster indexing because larger segments are more difficult to merge. [elasticsearch-in-action](https://weng.gitbooks.io/elasticsearch-in-action/content/chapter10_improving_performance/102optimizing_the_handling_of_lucene_segments.html)

- [段合并优化](https://learnku.com/articles/41593)

要注意的是，最新的官方文档有相关的建议：

> 只有在完成对索引的写入后，才应针对该索引调用强制合并。强制合并可能会导致生成非常大（>5GB）的段，如果继续写入此类索引，则自动合并策略将**永远不会考虑将来合并这些段(max_merged_segment参数： 5G)**，直到它们**主要由已删除的文档组成**。这会导致索引中**保留非常大的段**，从而导致**磁盘使用率增加和搜索性能降低**。

通过```GET /_cat/segments```获取到段的大小，可以看到即使没有手动进行段合并，也会因为更新频繁，导致segment的大小超出5G，也会造成性能问题。所以定时的`POST /appinfo/_forcemerge?only_expunge_deletes=true`还是可行的？但需要月度或者季度进行reindex操作更好，让segment分散？数据量较大则调整分片数目。


问题： 为什么正式环境里面的deleted docs那么多。按理来说正常的merge segment会把deleted docs清空。

A: 通过`GET /_cat/segments?index=material*&s=size:desc&v=true`可以看到，很多的segment已经得到5G，超过max_merged_segment的限制，所以这些segment不会参与到自动merge过程。而这些segment因为还会被经常更新，导致其deleted docs一直被增加，而无法得到回收?

问题: 手动执行`POST /appinfo/_forcemerge?only_expunge_deletes=true`会把这些segment合成一个？导致5G+5G这样的大segment么？

A:
