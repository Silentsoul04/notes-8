# kafka

标签（空格分隔）： kafka

---




kafka的feature特性？用来堆栈存放请求的？

最简单的提交方式是让悄费者自动提交偏移量。如果enable_auto_commit 被设为 true ，那么每过5秒，消费者会自动把从poll （） 方法接收到的最大偏移量提交上去。

kafka是怎么自动提交的？每过5秒提交？Python怎么发起的？
kafka 在每次pull_once的时候会去判断一下时间，如果每次时间适合了，会发起异步自动提交offset的操作。可以通过搜索一下相关配置的关键词找到相关的处理


 poll （） 方法能返回一个记录列表。每条记录都包含了记录所属主题的信息、记 在分区的信息 记录在分区里的偏移量 ，以及记录的键值对。我们 般会遍历这个列表 ，逐条处理这些记录。 
 
 poll （） 方法有一个超时参数 它指定了方法在多久之后可以返回，不管有没有可用 数据都要返回，可以看下kafka-python的poll源码。超时时间的设置取决于应用程序对响应速度的要求，比如要在多长 内把控制权归还给执行轮询的线程。
 
也就是timeout_ms参数，跟fetch_max_wait_ms是有区别的。这个是控制kafka多久返回一次消息，用于指定 broker 的等待时间，默认是 500ms 。

consumer_timeout_ms：若不指定consumer_timeout_ms，message iteration，默认一直循环等待接收，若指定，则超时返回，不再等待

fetch.min.bytes :这样可以降低消费者和 broker的工作负载，因为它们在主题不是很
跃的时候（或者一天里的低谷时段）就不需要来来回回地处理消息。如果没有很多可用
数据，但消费者的 CPU使用率却很高，那么就需要把该属性的值设得比默认值大。如果
消费者的数量比较多，把该属性的值设置得大一点可以降低 broker 的工作负载（避免过多的请求）。


---


消息被分批次写入 Kafka 批次就是一组消息，这些消息属于同一 主题和分区。减少网络开销


这要在时间延迟和吞吐量之间作出权衡：批次越大，单位时间内处理的消息就越多，单个消息的传输时间就越长。批次数据会被压缩，这样可以提高数据的传输和存储能力，但要做更多的计算处理。


**Avro** 提供了一种紧凑的序列化 格式，模式和消息体是分开的，当模式发生变 时，不需要重新生成代码 它还支持**强类型和模式进化**，其版本既向前兼容， 向后兼容。


### 主题和分区

kafka 的消息通过 主 题进行分类。主题就好比数据库的表，或者文件系统里的文件夹。主题可以被分为若干个分区 ， 一个分区就是一个提交日志。消息以追加的方式写入分区，然后以先入先出的顺序读取。由于一个主题一般包含几个分区，因此无法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。

kafka 通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器上，也就是说，一个主题可以横跨多个服务器，以此来提供比单个服务器更强大的性能。

在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper Kafka 上，如果消费者关闭或重 
启，它的读取状态不会丢失。

### broker和集群

一个独立的 Kafka 服务器被称为 broker broker 接收来自 生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。

在集群中， 一 个分区从属于 一 个 broker, i亥 broker 被称为分区的首领。一个分区可以分配给多个 broker ，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 broker 失效，其他 broker 可以接管领导权。


> 分区复制和选举

主题可以配置自己的保留策略，可以将消息保留到不再使用它们为止。可以通过配置把主题当作紧凑型日志， 只有最后一个带有特定键的消息会被保留下来。

### 为什么选择kafak

- 多个生产者。无需协调来自不同生成者的数据流
- 多个消费者。kafka 也支持多个消费者从一个单独的消息流上读取数据，而且消费者之间直不影响。这与其他队列系统不同，其他队列系统的消息一旦被一个客户端读取，其他客户端就无法再读取它。另外，多个消费者可以组成一个群组，它们共享一个消息流，并保证整个群组对每个给定的消息只处理一次。
- 基于磁盘的数据存储。非实时读取消息，持久化。而无需担心消息丢失或堵塞在生产者端 消费者可以被关闭，但消息会继续保留在 Kafka 里。消费者可以从上次中断的地方继续处理消息。
- 伸缩性。一个具有灵活伸缩性的系统，对在线集群进行扩展丝毫不影响整体系统的可用性。要提高集群的容错能力，需要配置较高的复制系数。 
- 高性能。上面提到的所有特性，让 Kafka 成为了一个高性能的发布与订阅消息系统。通过横向扩展生产者、消费者和 broker, Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟。

### 使用场景
- 活动追踪
- 传递消息
- 度量指标和日志记录
- 提交日志
- 流处理


P39: kafka的一些安装，实际简单例子和配置参数

### **如何选定分区数量**

原因： kafka 集群通过分区对主题进行横向扩展，所以当有新的broker 加入集群时，可以通过分区个数来实现集群的负载均衡。
当然，这并不是说，在存在多个主题的情况下（它们分布在多个 broker 上），为了能让分区分布到所有 broker 上， 主题分区的个数必须要大于 broker 的个数。不过，拥有大量消息的主题如果要进行负载分散，就需要大量的分区。

因素：

- 主题需要达到多大的吞吐量？例如，是希望每秒钟写入1OOKB 还是1GB?
- 从单个分区读取数据的最大吞吐量是多少？每个分区一般都会有一个消费者，如果你知道消费者将数据写入数据库的速度不会超过每秒50MB ，那
么你也该知道，从一个分区读取数据的吞吐量不需要超过每秒 50MB 。
- 可以通过类似的方法估算生产者向单个分区写入数据的吞吐量，不过生产者的速度一般比消费者快得多，所以最好为生产者多估算一些吞吐量。
- 每个 broker 包含的分区个数、可用的磁盘空间和网络带宽。
- 如果消息是按照不同的键来写入分区的，那么为已有的主题新增分区就会很困难。
- 单个broker对分区个数是有限制的，因为分区越多，占用的内存越多，完成首领选举需要的时间也越长。

例子：很显然，综合考虑以上几个因素，你需要很多分区，但不能太多。如果你估算出主题的吞吐量和悄费者吞吐量，可以用主题吞吐量除以消费者吞吐量算出分区的个数。也就是说，如果每秒钟要从主题上写入和读取 lGB 的数据，并且每个消费者每秒钟可以处理50MB的数据，那么至少需要 20 个分区。这样就可以让 20 个消费者同时读取这些分区，从而达到每秒钟 lGB 的吞吐量。

> 如果不知道这些信息，那么根据经验，把分区的大小限制在25GB 以内 可以得到比较理想的效果。


### 默认参数

根据时间保留数据是通过检查磁盘上日志片段文件的最后修改时间来实现的。


日志片段：
当消息到达 broker 时，它们被迫加到分区的当前日志片段上。当日志片段大小达到 log.segment bytes 定的上限（默认是 lGB ）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就会越频繁地关闭和分配新文件，从而降低磁盘写入的整体效率。


在日志片段被关闭之前消息是不会过期的。如果主题的消息量不大，那么如何调整这个参数的大小就变得尤为重要。例如，如果主题每天只接收 lOOMB 的消息，而 log.segment.bytes 使用默认设置，那么需要 10 天时间才能填满一个日志片段。


log.segment.ms指定了 多长时间之后日志片段会被关闭。


broker 通过设置 message.max.bytes 参数来限制单个消息的大小，默认值是1 000 000 ，也就是 1MB 。跟其他与字节相关的配置参数一样 ，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 message.max.bytes 指定的值，消息的实际大小可以远大于这个值。


服务器端可用的内存容量是影响客户端性能的主要因素。磁盘性能影响生产者 ，而内存影响消费者.
消费者 般从分区尾部读取消息，如果有生产者存在，就紧跟在生产者后面。在这种情况下，消费者读取的消息会直接存放在系统的页面缓存里，这比从磁盘上重新读取要快得多。


### 需要多少个broker

- 首先，需要多少磁盘空间来保留数据，以及单个 broker 有多少空间可用。如果启用了数据复制，那么至少还需要一倍的空间。
- 集群处理请求的能力。这通常与网络接口处理客户端流量的能力有关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量爆发）时。如果单个 broker 的网络接口在高峰时段可以达到80%的使用量，并且有两个消费者，那么消费者就无法保持峰值，除非有两个 broker 。。如果集群启用了复制功能，则要把这个额外的消费者考虑在内。因磁盘吞吐量低和系 内存不足造成的性能问题，也可以通过扩展多个 broker 来解决。


建议使用最新版本的 Kafka ，让消费者把偏移量提交到 Kafka 服务器上，消除对 **Zookeeper** 的依赖.


## 生产者


服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka ，就返回RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败， 则 会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败， 就返回错误信息。



### 生产者的一些配置

到达副本数，内存缓冲区大小，压缩算法，重试，批次消息发送大小，批次等待时间、单个消息的最大值。TCP socke 接收和发送数据包的缓冲区大小


参考链接: 

[kafka生产者](./kafka生产者.md)


---



Avro 数据通过与语言无关 schema 来定义 schema 通过 JSON 来描述，数据被序列化
成二进制文件或 JSON 文件，不过 般会使用 进制文件。 Avr。在读写文件时需要用到
schema, schem 般会被内嵌在数据文件里。


用 Avro 的好处：我们修改了消息的 schema ，但并没有更新所有负责
取数据的应用程序，而这样仍然不会出现异常或阻断性错误，也不需要对现有数据进行大幅更新。但有些兼容性原则。默认值？


我们把所有写人数据需要用到的 sc hem 保存在注册表里，然后在记录里引用 schema 的标
识符。负责读取数据的应用程序使用标识符从注册表里拉取 schema 来反序列化记录。


Kafka 消费者从属于消费者群组。一个群组里的消费者订阅的是同 个主题，每个消费
接收主题 部分分区的消息。


每个消费者只处理部分分区的消息，这就是横向伸缩的主要手段。我们有必要为主题
创建大量的分区，在负载增长时可以加入更多的消费者


一个新的悄费者加
入群组时，它读取的是原本由其他消费者读取的消息。当一个消费者被关闭或发生崩愤
时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。分区的所有权从 个消费者转移到另 个消费者，这样的行为被称为再均衡。


在再均衡期间，消费者无陆读取消
息，造成整个群组 小段时间的不可用。另外，当分区被重新分配给另 个消费者时，消
费者当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序

消费者通过向被指派为群组协调器的 broker （不同的群组可以有不同的协调器）发送心跳
来维持它们和群组的从属关系以及它们对分区的所有权关系。

正则表达式可以匹配多个主
题， 如果有人创建了新的主题，并且主题的名字与正则表达式匹配，那么会立即触发
再均衡，消费者就可以读取新添加的主题。

消息轮询是消费者 API的核心，通过个简单的轮询向服务器请求数据。一旦消费者订阅了主题，轮询就处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，开发者只需要使用一组简单的 API 来处理从分区返回的数据。


线程安全：按照规则， 一个 消费者使用 个线程


配置：最小发送大小，最小发送时间间隔，回话过期时间，提交偏移量的方式，分区分配策略

从每个分区里返回给消费者的最大字节数:
肖费者需要频繁调用 poll （） 方陆
来避免会话过期和发生分区再均衡，如果单次调用 poll （） 返回的数据太多，消费者需要更
多的时间 处理，可能无怯及时进行下 个轮询来避免会话过期。如果出现这种情况，
以把 J11ax.pa ti.ti.on.fetch.bytes 值改 ，或者延长会 舌过期时间。

heartbeat.interval.ms 指定了 poll （） 方住向协调
发送心跳的频率， session.timeout. ms 则指定了消费者可以多久不发送心跳。所以，
般需要同时修改这两个属性， heartbeat.interval.ms 必须比 session.timeout. ms 小，
般是 session.timeout. ms 三分之一 。

auto.offset.reset
该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时井被删除）该作何处理。


max.poll.records
单次调用 call （） 方住能够返回的记录数量，可以帮你控制在轮询里需要处
理的数据量。


我们把更新分区当前位置的操作叫作提交。消费者往一个叫作 consumer_offset 特殊主题发送消息，消息里包含每个分区的偏移量。如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复处理


自动提交会出现重复悄息的时间窗。
在使用自动提交 ，每次调用轮询方怯 上一次调 的偏移量 交上去，它并不
知道具体哪些消息已经被处理了，所以在再次调用之前最好 有当 调用返回 消息
都已经处理完毕


CommitSync （） 将会提交由 poll （） 返回的最新偏移量 理完所有记录后要
确保调用了 CommitSync（），否则还是会有丢失消息的风险。如果发生了再均衡，从最近一批消息到发生再均衡之间的所有消息都将被重复处理。手动提交有一个不足之处，在 broker 对提交请求作出回应之前，应用程序会直阻塞，这样会限制应用程序的吞吐量。我 可以通过降低提交频率来提升吞吐盆，但如果发生了均衡，会增重复消息的数量。


CommitAsync只管发送提交请求，无需等待 broker 的响应。为 CommitAsync （）也支持因调，在 broker作出响应时会执行回调。回调经常被用于记录提交错误或生成度量指标，过如果你要用它来进行重试， 一定要注意提交的顺序。

在进行重试前，先检查回调的序列号和即将提交的偏移量是否相等，如果相等，说明没有新的提交，那么可以安全地进行重试。如果序列号比较大，说明有一个新的提交已经发送出去了，应该停止重试。

同步和异步组合提交：
一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，因为如果提交失因为临时问题导致的，那么后续的提交总会有成功的。但如果这是发生在关闭消费者均衡前的最后 次提交，就要确保能够提交成功。

消费者 API 允许在调用CommitSync（）和CommitAsync（）方桂时传进去希望提交
分区和偏移量的 map


再均衡监昕器：在为消 费者分配新分区或移除旧分区 时，可以通过消 费者 PI 执行 些应用程序代码，在调用 subscribe （） 方法时传进去 Consume Rebalancelistener 「实例就可以了。Consume Rebalancelistener 「有两个需要实现的方法。


原子性操作：
如果偏移量是保存在数据库里而不是 Kafka 里，那么消费者在得到新分区
时怎么知道该从哪里开始读取？这个时候可以使用 seek （） 方战。在消费者启动或分配到新分区时 ，可以使用 seek （）方告 找保存在数据库里的偏移量。


要记住， Consumer.wakeup（） 是消费
一一个可以从 线程里安全调 的方法。调用 Consumer.wakeup（）可以退出 poll()
并抛出一个WakeupException 异常，或者如果调用Consumer.wakeup（）时线程没有等待轮询，
那么异常将在下 轮调用 poll （）时抛出。


一个 消费者可以订阅主题（井加入消费者群组），或者为自己分配分区 但不 时做这两件事情。


Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集
群时通知控制器。控制器负责在节点加入或离开集群时进行**分区首领**选举。控制器使用epoch 来避免“脑裂”。“脑裂”是指两个节点同时认为自己是当前的控制器。


首领副本每个分区都有一个首领副本为了保证致性，所有生产者请求和消费者请求都会经过这个副本。跟随者副本首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领发生崩渍，其中的跟随者会被提升为新首领。

首领的另一个任务是搞清楚哪个跟随者的状态与自己是一致的。如果跟随者在 10 内没有请求任何消息，或者虽然在请求消息，但在10s内没有请求最新的数据，那么它就会被认为是不同步的。

持续请求得到的最新悄息副本被称为同步的副本。在首领发生失效时，只有同步副本才有可能被选为新首领。


Kafka 客户端要自己负责把生产请求和获取请求发送到正确的broker 上。。元数据请求可以发送给任意一个 broker ，因为所有 broker 都缓存了这些信息。刷新的时间间隔通
metadata.max.age.Ms 参数来配置


Kafka 使用零复制技术向客户端发送消息一一也就是说， Kafka 直接把消
息从文件（或者更确切地说是Linux文件系统缓存）里发送到网络通道，而不需要经过任何中间缓冲区。其他数据库在将数据发送给客户端之前会先把它们保存在本地缓存里。这项技术避免了字节复制，也不需要管理内存缓冲区，从而获得更好的性能。


并不是所有保存在分区首领上的数据都可以被客户端读取,因为还没有被足够多副本复制的消息被认为是“不安全”的,如果broker间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长。

分区分配测量，计算每个目录里的分区数量，新的分区总是被添加
到数量最小的那个目录里。

因为在 个大文件里查找和删除消息是很费时的，也很容易出错，所以我们把分区分成若个片 默认情况下，每个片段包含lGB或者1周的数据，以较小的那个为准。在 broker往分区写入数据时，如果达到片段上限，就关闭当前文件，井打开 个新文件。当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。


为了帮助 broker 更快地定位到指定的偏移量， Kafka为每个分区维护了 个索引。索引把偏移量映射到片段文件和偏移量在文件里的位置。索引也被分成片段，所以在删除消息时，也可以删除相应的索引Kafka不维护索引的校验和。如果索引出现损坏，Kafka会通过重新读取消息并录制偏移量和位置来重新成索引。如果有必要，管理员可以删除索引，这样做是绝对安全的， Kafka 会自动重新生成这些索引。


为了清理分区 ，情理线程会读取分区的污浊部分，井在内存里创建 map map 里的 每个元素 含了消息键的散列值和消息的偏移量，键的散列值是 16B ，加上偏移量总共是24B 如果要清理 lGB 的日志片段，并假设每个消息大小为 阻，那么这个片段就包 含一百万个悄息，而我们只需要用 24MB map 就可以清理这个片段。（如果有重复的键，可以重用散列项，从而使用更少的内存。）这是非常高效的！


为了彻底把 个键从系统里删除，应用程序必须发送 个包含该键且值为 null 的消息。清理线程发现该悄息时，会先进行常规的清理，只保留值为 null 的消息。该悄息（被称为墓碑消息）会被保留 段时间，时间长短是可配置的。


Kafka 可以在哪些方面作出保证呢？
- Kafka 可以保证分区消息的顺序。
- 只有当消息被写入分区的所有同步副本时（但不一定要写入磁盘），它才被认为是“提交”的
- 只要还有 个副本是活跃的，那么已经提交的消息就不会丢失
- 消费者只能读取已经提交的悄息。

消费者唯一要做的是跟踪哪些消息是已经读取过的，哪些是还没有读取过的。这是在读取消息时不丢失消息的关键。

尽管 Kafka 现在还不能完全支持仅一次语义。，要么消息本身包含一个唯一键（通常都是这样），要么使用主题、分区和偏移量的组合来创建唯唯一键－它们的组合可以唯一标识Kafka记录。如果你把消息和一个唯一键写入系统，然后碰巧又读到个相同的消息，只要把原先的键值覆盖掉即可。**幂等性写入**

如果写入消息的系统支持事务，那么就可以使用另一种方棒。将偏移量写入数据库中， 然后调用 seek （）方也从该偏移量位置继续读取数据。


Kafka 的代码库里包含了大量测试用例，考虑运行以下一些测试。

- 首领选举 ：如果我停掉首领会发生什么事情？生产者和消费者重新恢复正常状态需要长时间？
- 控制器选举 重启控制器后系统需要多少时间来恢复状态？
- 依次重启：可以依次重启 broker 不丢失任何数据吗？
- 不完全首领选举测试：如果依次停止所有副本（确保每个副本都变为不同步的），然启动一个不同步的broker会发生什么？要怎样恢复正常？这样做是可接受的吗？

对于生产者来说，最重要的两个可靠性指标是消息的巳汀 error- rate retry-rate （聚合过的）。

对于消费者来说，最重要的指标是 consumer lag ，该指标表一了消费者的处理速度与最近提交到分区里的偏移量之间还有多少差距。理想情况下，该指标总是为 ，消费者总能读到最新的消息。


Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。 Kafka 的解藕能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择。

数据管道需要协调各种数据格式和数据类型。

数据管道的构建可以分为两大阵营，即 ETL ELT。

ETL当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。有可能给数据管道造成不适当的计算和存储负担。下游得到的数据不是完整的，如果它们想要访问被移除的字段，只能重新构建管道，井重新处理历史数据（如果可能的话）。


ELT：在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。这种情况也被称为高保真（ high fidelity ）数据管道或数据湖（data lake ）架构。目标系统收集
“原始数据”，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。这种方式的不足在于，数据的转换占用了目标系统太多的 CPU 和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统。


连接器和任务负责“数据的移动”， worker 进程负责 REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡。

编写代码从 Kafka 读取数据并将其插入数据库只需一到两天的时间，但是如果要处理好配置、异常、 REST API 、监控、部署、伸缩、失效等问题，可 需要几个月 如果你使用连接器来实现数据复制，连接器插件会为你处理掉大堆复杂的问题

源连接器所做的事情都很相似一一－ 系统读 事件，并为每个事件生成
schem 和值（值就是数据对象本身） 目标连接器正好相反，它们获取 schema 和值，井使schema 来解析值，然后入到目标系统。中间可以选择使用合适的转化器，用于将数据保存到 Kafka。

在设计一个源连接器时，要着重考虑如何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的井行能力，也决定了连接器是否能够实现至少一 次传递或者仅一次传递。

可靠性是数据集成系统唯一一个重要的需求

#### Hub Spoke
一个中心 Kafka 集群对应多个本地Kafka集群的情况。在采用这种架构时，每个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程会读取每 个区域数据中心的数据，并将它重新生成到中心集群上。

因为数据复制是单向的，而且消费者总是从同一个集群读取数据，所以这种架构易于部署、配置和监控。这种架构模式在数据访问方面有所局限，因为区域数据中心之间的数据是完全独立的。


#### 双活架构
这种架构的主要好处在于，它可以为就近的用户提供服务，具有性能上的优势，而且不会因为数据的可用性问题（在 Hub Spoke架构中就有这种问题）在功能方面作出牺牲。第二个好处是冗余和弹性。因为每个数据中心具备完整的功能，一个数据中心发生失效，就可以把用户重定向到另一个数据中心。这种重定向完全是网络的重定向，因此是一种最简单、最透明的失效备援方案。

这种架构的主要问题在于，如何在进行多个位置的数据异步读取和异步更新时避免冲突。

双活镜像（特别是当数据中心的数量超过两个）的挑战之处在于，每两个数据中心之间需要进行镜像，而且是双向的。另外，我们还要避免循环镜像，相同的事件不能无止境地来回镜像。


#### 主备架构
冷备。你可以安装第二个集群，然使用镜像进程将第一个集群的数据完整镜像到第二个集群上，不需要担心数据的访问和冲突问题，也不需要担心它会带来像其他架构那样的复杂性。这种架构的不足在于，它浪费了一个集群。 Kafka 集群间的失效备援比我们想象的要**难得多**。从目前的情况来看，要实现不丢失数据或无重复数据的 Kafka 集群失效备援是不可能。

有些组织则倾向于让灾备集群在平常也能发挥作用，他们把些只读的工作负载定向到灾备集群上，也就说，实际上运行的是 Hub Spoke 架构的一个简化版本，因为架构里只有一个Spoke。


目前 Kafka 不支持事务，那么在失效备提过程中，一些数据可以及时到达灾
备集群，而有些则不能。那么在切换到灾备集群之后，应用程序需要知道该如何处理没有相关销售信息的产品数据。


在切换到灾备集群的过程中，最具挑战性的事情莫过于如何让应用程序知道该从什么地方开始继续处理数据：
要么从头开始取数据，并处理大量的重复数据，要么直接跳到末尾，放弃一些数据（希望只是少量数据）
复制偏移量主题，，消费者会把偏移量提交到一个_consurner_offsets 的主题上。如果对这个主题进行了镜像。首先， 我们并不能保证主集群里的偏移量与灾备集群里的偏移量是完全匹配 。即使如此生产者在后续进行重试时仍然会造成偏移量的偏离。简而言之，目前的 Kafka 镜像解决方案无怯为主集群和灾备集群保留偏移量。

用于根据时间戳查找偏移量。于是，假设你正在进行失效备援 井且知道失效事件发生在凌 4:05 ，那么就可以让消费或者从 4: 03 的位置开始处理数据。

我们知道，镜像偏移量主题的一个最大问题在于主集群和灾备集群的偏移量会发生偏差。因此，一些组织选择使用外部数据存储（比如 Apache Cassandra 来保存集群之间的偏移量映射。这种方案非常复杂，我认为并不值得投入额外的时间。

但在今天，我倾向于将集群升级到新版本，并使用基于时间戳的
解决方案，而不是进行偏移量映射，更何况偏移量映射并不能覆盖所有的失效备援场景。

最简单的解决方案是清理旧的主集群，删掉所有的数据和偏移量，然后
从新 主集群上把数据镜像回来，这样可以保证两个集群的数据是一致的。


首先，延展集群井非多个集群，而是单个集群，因此不需要对延展集群进行镜像。延展集群使用 Kafka 内置的复制机制在集群的broker 之间同步数据。

这种架构的不足之处在于，它所能应对的灾难类型很有限，只能对数据中心的故障，无法应对应用程序或者 Kafka 故障。运维的复杂性是它的另 个不足之处，它所 要的物基础设施并不是所有公司都能够承担得起的。


MirrorMaker 为每个消费者分配一个线程，消费集群的主题和分区上读取数据，然后通过公共生产者将数据发送到目标集群上


如果说broker 只有一个可监控的度量指标，那么它 定是指非同步分区的数量。 该度量指明了作为首领的broker 有多少个分区处于非同步状态。


如果多个 broker 都出现了非同步分区， 那么有可能是集 的问题，也有可能是单个 broker的问题。这时候有可能是因为 broker 无陆从其 broker 那里复制数据。为了找出这个、broker ，可以列出集群的所有非同步分区 ，井检查它们的共性.

- 网络输入吞吐量。
- 网络输出吞吐量。
- 磁盘平均等待时间。
- 磁盘使用百分比。

任何时候，都应该只有 broker 是控制器，而且这个 broker必须一直是集群控制器。如果出现了两个控制器，说明有 个本该退出的控制器线程被阻塞了，这会导致管理任务无陆正常执行，比如移动分区。

请求处理器平均空闲百分比这个度量指标表示请求处理器空闲时间的百分比。数值越低，说明 broker 的负载越高。


第一种是 kafka.controller，可以将它设置为 INFO 。这个日志     用于记录集群控制器的信息。在任何时候，集群里都只有一个控制器，因此只有一个 broker会使用这个日志。日志里包含了主题的创建和修改操作、 broker 状态的变更，以及集群的活动，比如默认的副本选举和分区的移动。

另一个日志是 kafka.se ver.ClientQuotaManager也可以将它设置为 INFO 级别。这个日志用于记录与生产和消费配额活动相关的信息。因为这些信息很有用，所以最好不要把它们记录在 broker 的主日志文件里。

我们可以对 equest-latency avg 设置告警，它表示发送一个生产者请求到 broker 所需要的平均时间。

事件流的特点：

- 事件流是有序
- 不可变的数据记录:事件一旦发生，就不能被改变。个金融交易被取消，并不是说它就消失了，相反，这需要往事件流里添 个额外的事件，表示前 个交易的取消操作。
- 事件流是可重播的,，但对于大多数业务来说，重播发生在几个月前（甚至几年前）的原始事件流是个很重要的需求。可能是为了尝试使用新的分析方法纠正过去的错误，或是为了进行审计。


P206
有时候，流式处理需要将外部数据和流集成在起，比如使用保存在外部数据库里的规则来验证事务，或者将用户信息填充到点击事 中。

为了获得更好的性能和更强的伸缩性，需要将数据库的信息缓存到流式处理 用程序里。不过，要管理好这个缓存也是一个挑战。

如果刷新太频繁，那么仍然会对数据库造成压力，缓存也就失去了作用。如 刷新不及时， 那么流式处理中所用的数据就会过时。

如果能够捕捉数据库的变更事件，井形成事件流，流式处理作业就可以监昕 件流， 井及时更新缓存。捕捉数据库的变更事件井形成事件流，这个过程被称为 CDC 变更数据捕捉（Change Data Capture）。


时间或许就是流式处理最为重要的概念，也是最让人感到困惑的。fcuk延迟数据到达：事件时间、日至追加时间、处理时间

事件与事件之间的信息被称为“状态"

为了将流转化成表， 需要“应用”流里所包含的所有变更，这也叫作流的“物化”。首先在 内存里 内部状态存储或外部数据库里创建 个表，然后从头到尾遍历流里的所
有事件，逐个地改变状态。在完成这个过程之后，得到了个表，它代表了某个时间点的状态。

如果“移动间隔”与窗口大小相等，这种情况被称为“滚动窗口”（tumbling window ）”。如果窗口随着每一条记录移动，这种情况被称为“滑动窗口（sliding dow ）”。跳跃窗口，每分钟统计近5分钟的数据。


窗口的可更新时间是多长。理想情况下，可以定义一个时间段，在这个时间段内， 事件可以被添加到与它们相应的时间片段里。如果事件处于 4个小时以内，那么就更新它们就忽略它们。

乱序的事件如何处理 P195
它们在本地状态里维护了多个聚合时间窗口，用于更新事件，并为开发者提供配置时间窗口大小的能力。当然，时间窗口越大，维护本地状态需要的内存也越大。

哪怕是一个很简单的应用，都需要一个拓扑。拓宁｜、是由处理器组成的，这些处理器是拓扑图里的节点（用椭圆表示）。大部分处理器都实现了一个数据操作一 过滤、映射、聚合等。数据源处理器从主题上读取数据，井传给其他组件 而数据地处理器从上一个处理器接收数据，并将它们生成到主题上。拓扑总是从 个或多个数据源处理器开始，井以或多个数据地处理器结束。